# 英伟达发布新人工智能芯片，CEO黄仁勋：大模型运行成本将大幅降低

# 英伟达发布新人工智能芯片，CEO黄仁勋：大模型运行成本将大幅降低

![](https://inews.gtimg.com/news_bt/OSEQarJyBx9UNjec-1jjsYjC2XYid23_AC11wNSzhYXwsAA/1000)

腾讯科技讯
8月9日消息，美国当地时间周二晚间，英伟达创始人兼首席执行官黄仁勋在2023年度计算机图形学大会（SIGGRAPH）上发表演讲，讨论该公司最新的人工智能和3D图形领域的创新。

据估计，目前英伟达在AI芯片市场占据了超过80%的市场份额。该公司的专长是图形处理单元（GPU），已成为支撑生成式人工智能软件（如谷歌的Bard和OpenAI的ChatGPT）的大型AI模型的首选芯片。但由于科技巨头、云服务提供商和初创企业争夺GPU容量来开发自己的AI模型，英伟达的芯片供应短缺。

英伟达最强AI超算再升级

英伟达最新发布的芯片GH200，与该公司目前最高端的AI芯片H100采用相同的GPU。但GH200还配备了141GB的尖端内存和一个72核的ARM中央处理器。

相比前代平台，新GH200 Grace Hopper平台的双芯片配置将内存容量提高3.5倍，带宽增加三倍，一个服务器就有144个Arm
Neoverse高性能内核、8 petaflops 的 AI 性能和282GB的最新HBM3e内存技术。

HBM3e是一种高带宽内存，带宽达每秒5TB。该内存比当前的HBM3快50%，可提供总共每秒10TB的组合带宽，使新平台能运行比前代大3.5倍的模型，同时通过快三倍的内存带宽提高性能。

英伟达CEO黄仁勋在周二的演讲中表示：“我们给这个处理器加了一些提升。”他补充说：“这个处理器是为全球数据中心的规模而设计的。”

据黄仁勋透露，这款新芯片将于明年第二季度在英伟达的经销商处上市，并计划在年底前提供样品。英伟达的代表拒绝透露价格。

通常，处理AI模型的过程至少分为两个部分：训练和推理。

首先，使用大量数据对模型进行训练，这个过程可能需要数月时间，有时需要数千个GPU，例如英伟达的H100和A100芯片。

然后，该模型将在软件中用于进行预测或生成内容，这个过程称为推理。与训练类似，推理需要大量的计算资源，并且每次软件运行时都需要大量的处理能力，例如生成文本或图像时。但与训练不同，推理是几乎持续进行的，而训练只在需要更新模型时才需要进行。

**黄仁勋表示：“你可以将几乎任何大型语言模型放入其中，它将进行疯狂的推理。大型语言模型的推理成本将大幅降低。”**

英伟达的新芯片GH200专为推理而设计，因为它具有更大的内存容量，可以容纳更大的AI模型在单个系统中。英伟达副总裁伊恩·巴克在与分析师和记者的电话会议上表示，英伟达的H100具有80GB的内存，而新的GH200则有141GB的内存。英伟达还宣布推出了一种将两个GH200芯片组合到一台计算机中的系统，以支持更大的模型。

巴克表示：“更大的内存使得模型可以常驻在单个GPU上，而不需要多个系统或多个GPU来运行。”

此次宣布正值英伟达的主要GPU竞争对手AMD推出面向AI的芯片MI300X，该芯片支持192GB的内存，并被市场推广为适用于AI推理。谷歌和亚马逊等公司也正在设计自己的定制AI芯片用于推理。

RTX工作站：4款显卡齐上新

除了GH200，英伟达的桌面AI工作站GPU系列也全面上新，一口气推出了4款新品：RTX 6000、RTX 5000、RTX 4500和RTX
4000。针对企业客户，英伟达还准备一套一站式解决方案—— RTX Workstation，支持最多4张RTX 6000 GPU。

OVX服务器：搭载L40S，性能小胜A100

针对数据中心市场，英伟达推出了最多可搭载8张L40S
GPU的OVX服务器。据介绍，对于具有数十亿参数和多种模态的生成式AI工作负载，L40S相较于老前辈A100可实现高达1.2倍的推理性能提升，以及高达1.7倍的训练性能提升。

AI Workbench：加速定制生成式AI应用

除了各种强大的硬件之外，英伟达还发布了全新的AI Workbench，来帮助开发和部署生成式AI模型。AI
Workbench为开发者提供了一个统一且易于使用的工具包，能够快速在PC或工作站上创建、测试和微调模型，并扩展到几乎任何数据中心、公有云或英伟达的云服务DGX
Cloud上。

具体而言，AI
Workbench具备易于使用、集成AI开发工具和存储库、增强协作、访问加速计算资源等优势（老黄深夜炸场，AIGC进入iPhone时刻，神秘显卡胜过A100）。

五年前悄然押注AI获得丰厚回报

黄仁勋表示，英伟达在2018年做出了一个攸关公司生死存亡的商业决定，但却很少有人意识到这个决定将对英伟达乃至整个半导体行业的重大意义。当然，这一押注为英伟达带来了丰厚的回报，但黄仁勋表示，这只是人工智能驱动的未来的开端，一个主要由英伟达硬件驱动的未来。

黄仁勋回忆说，五年前的分水岭时刻是选择采用以光线追踪（RTX）还是智能升级（DLSS）形式的人工智能图像处理方案。“我们意识到栅格化正在达到极限，”他说，他指的是传统的、被广泛使用的3D场景渲染方法。“2018年是一个‘押注公司’的时刻。这需要我们重新发明硬件、软件和算法。在我们用人工智能重塑CG的同时，我们也在为人工智能重塑GPU。”

虽然光线跟踪和DLSS仍在消费者GPU和游戏多样化等领域被采用，但他们为实现它而创建的架构却是不断增长的机器学习开发社区的完美合作伙伴。

训练越来越大的生成式模型所需的大量计算，不再由某些具有GPU功能的传统数据中心提供，而是从一开始就设计用于执行必要大规模操作的系统支持，如H100。公平地说，人工智能的发展在某些方面只是受到这些计算资源可用性的限制。英伟达从人工智能热潮中受益，其服务器和工作站的销量始终处于供不应求的状态。

但黄仁勋坚称，这只是个开始。新的模型不仅需要训练，还需要数百万甚至数十亿用户的实时运行支持。他说：“大语言模型在未来几乎处于一切事物的前沿：从视觉效果到快速数字化的制造市场、工厂设计和重工业，一切都将在某种程度上采用自然语言界面。整个工厂将由软件控制的机器人操控，他们将制造的汽车本身也将是机器人。所以这是机器人设计机器人，并负责建造机器人。”

有些人可能不同意黄仁勋的观点，这虽然看似合理，但碰巧也非常有利于英伟达的利益。

但是，尽管我们对大语言模型的依赖程度尚不确定，但很少有人敢说不使用它们，甚至对谁将使用它，以及在什么情况下需要在新的计算资源上进行重大投资方面也是如此。

在以CPU为核心的机架等上一代计算资源上投资数百万美元已经没有意义，因为像GH200这样的硬件，可以用不到十分之一的成本和电力需求来完成同样的工作。GH200是英伟达新推出的、专用于数据中心的人工智能开发硬件。

黄仁勋还播放了一段视频，展示了由多个Grace
Hopper计算单元组成的乐高积木般的组件，它们先是组成了刀片，然后是机架，最后是成排的GH200，所有这些计算单元都被高速连接在一起，成为“世界上最大的单个GPU”，它拥有完整的ML专业计算能力。

黄仁勋提出，这些将成为未来人工智能主导的数字行业的基本单位。他说：“我不知道是谁说的，但你买的越多，省的钱就越多。如果我能让你们记住我今天演讲中的一件事，那就是它了。”

黄仁勋在演讲中没有提到人工智能面临的诸多挑战、监管，也没有谈到人工智能的整体概念在发生变化，
就像它在去年已经多次经历过的那样。当然，这是一种乐观的世界观，就像在淘金热期间出售镐和铲子等工具的商人，他们同样赚得盆满钵满。

